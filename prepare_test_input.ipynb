{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook is used to transform test data input to the format our model could apply to.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import glob\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "import scipy.special\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud, Box, Quaternion\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix\n",
    "\n",
    "import time\n",
    "from lyft_dataset_sdk.utils.map_mask import MapMask\n",
    "from pathlib import Path\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset,LyftDatasetExplorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lyft SDK requires creating a link to input folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ./images\n",
    "!rm ./maps\n",
    "!rm ./lidar\n",
    "!ln -s /home/jupyter/project/data/test_images images\n",
    "!ln -s /home/jupyter/project/data/test_maps maps\n",
    "!ln -s /home/jupyter/project/data/test_lidar lidar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new test data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyftTestDataset(LyftDataset):\n",
    "    \"\"\"Database class for Lyft Dataset to help query and retrieve information from the database.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, json_path: str, verbose: bool = True, map_resolution: float = 0.1):\n",
    "        \"\"\"Loads database and creates reverse indexes and shortcuts.\n",
    "        Args:\n",
    "            data_path: Path to the tables and data.\n",
    "            json_path: Path to the folder with json files\n",
    "            verbose: Whether to print status messages during load.\n",
    "            map_resolution: Resolution of maps (meters).\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_path = Path(data_path).expanduser().absolute()\n",
    "        self.json_path = Path(json_path)\n",
    "\n",
    "        self.table_names = [\n",
    "            \"category\",\n",
    "            \"attribute\",\n",
    "            \"sensor\",\n",
    "            \"calibrated_sensor\",\n",
    "            \"ego_pose\",\n",
    "            \"log\",\n",
    "            \"scene\",\n",
    "            \"sample\",\n",
    "            \"sample_data\",\n",
    "            \"map\",\n",
    "        ]\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Explicitly assign tables to help the IDE determine valid class members.\n",
    "        self.category = self.__load_table__(\"category\")\n",
    "        self.attribute = self.__load_table__(\"attribute\")\n",
    "        \n",
    "        \n",
    "        self.sensor = self.__load_table__(\"sensor\")\n",
    "        self.calibrated_sensor = self.__load_table__(\"calibrated_sensor\")\n",
    "        self.ego_pose = self.__load_table__(\"ego_pose\")\n",
    "        self.log = self.__load_table__(\"log\")\n",
    "        self.scene = self.__load_table__(\"scene\")\n",
    "        self.sample = self.__load_table__(\"sample\")\n",
    "        self.sample_data = self.__load_table__(\"sample_data\")\n",
    "        \n",
    "        self.map = self.__load_table__(\"map\")\n",
    "\n",
    "        if verbose:\n",
    "            for table in self.table_names:\n",
    "                print(\"{} {},\".format(len(getattr(self, table)), table))\n",
    "            print(\"Done loading in {:.1f} seconds.\\n======\".format(time.time() - start_time))\n",
    "\n",
    "        # Initialize LyftDatasetExplorer class\n",
    "        self.explorer = LyftDatasetExplorer(self)\n",
    "        # Make reverse indexes for common lookups.\n",
    "        self.__make_reverse_index__(verbose)\n",
    "        \n",
    "    def __make_reverse_index__(self, verbose: bool) -> None:\n",
    "        \"\"\"De-normalizes database to create reverse indices for common cases.\n",
    "        Args:\n",
    "            verbose: Whether to print outputs.\n",
    "        \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        if verbose:\n",
    "            print(\"Reverse indexing ...\")\n",
    "\n",
    "        # Store the mapping from token to table index for each table.\n",
    "        self._token2ind = dict()\n",
    "        for table in self.table_names:\n",
    "            self._token2ind[table] = dict()\n",
    "\n",
    "            for ind, member in enumerate(getattr(self, table)):\n",
    "                self._token2ind[table][member[\"token\"]] = ind\n",
    "\n",
    "        # Decorate (adds short-cut) sample_data with sensor information.\n",
    "        for record in self.sample_data:\n",
    "            cs_record = self.get(\"calibrated_sensor\", record[\"calibrated_sensor_token\"])\n",
    "            sensor_record = self.get(\"sensor\", cs_record[\"sensor_token\"])\n",
    "            record[\"sensor_modality\"] = sensor_record[\"modality\"]\n",
    "            record[\"channel\"] = sensor_record[\"channel\"]\n",
    "\n",
    "        # Reverse-index samples with sample_data and annotations.\n",
    "        for record in self.sample:\n",
    "            record[\"data\"] = {}\n",
    "            record[\"anns\"] = []\n",
    "\n",
    "        for record in self.sample_data:\n",
    "            if record[\"is_key_frame\"]:\n",
    "                sample_record = self.get(\"sample\", record[\"sample_token\"])\n",
    "                sample_record[\"data\"][record[\"channel\"]] = record[\"token\"]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Done reverse indexing in {:.1f} seconds.\\n======\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "17 attribute,\n",
      "8 sensor,\n",
      "168 calibrated_sensor,\n",
      "219744 ego_pose,\n",
      "218 log,\n",
      "218 scene,\n",
      "27468 sample,\n",
      "219744 sample_data,\n",
      "1 map,\n",
      "Done loading in 3.8 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 1.4 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "class_heights = {'animal':0.51,'bicycle':1.44,'bus':3.44,'car':1.72,'emergency_vehicle':2.39,'motorcycle':1.59,\n",
    "                'other_vehicle':3.23,'pedestrian':1.78,'truck':3.44}\n",
    "level5data = LyftTestDataset(data_path='.', json_path='./data/test_data', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7eb3e546df5311b035f1d4b7e88351ffdd85f311ef1bdd...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>a76d702996a086ecd4add92b6c533b76f0ab1dd35a4729...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>a9919963e5cd83d12b60bb9c386869ecb4dffcf2e3c401...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>e3f42dc16cb366723699a5e12098edb56907d676cb5720...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>033edef20c7f951738c037c0275dc02279a15454fda3e6...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Id  PredictionString\n",
       "0  7eb3e546df5311b035f1d4b7e88351ffdd85f311ef1bdd...               NaN\n",
       "1  a76d702996a086ecd4add92b6c533b76f0ab1dd35a4729...               NaN\n",
       "2  a9919963e5cd83d12b60bb9c386869ecb4dffcf2e3c401...               NaN\n",
       "3  e3f42dc16cb366723699a5e12098edb56907d676cb5720...               NaN\n",
       "4  033edef20c7f951738c037c0275dc02279a15454fda3e6...               NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub = pd.read_csv('./data/sample_submission.csv')\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformation_matrix_to_voxel_space(shape, voxel_size, offset):\n",
    "    \"\"\"\n",
    "    Constructs a transformation matrix given an output voxel shape such that (0,0,0) ends up in the center.\n",
    "    Voxel_size defines how large every voxel is in world coordinate, (1,1,1) would be the same as Minecraft voxels.\n",
    "    \n",
    "    An offset per axis in world coordinates (metric) can be provided, this is useful for Z (up-down) in lidar points.\n",
    "    \"\"\"\n",
    "    \n",
    "    shape, voxel_size, offset = np.array(shape), np.array(voxel_size), np.array(offset)\n",
    "    \n",
    "    tm = np.eye(4, dtype=np.float32)\n",
    "    translation = shape/2 + offset/voxel_size\n",
    "    \n",
    "    tm = tm * np.array(np.hstack((1/voxel_size, [1])))\n",
    "    tm[:3, 3] = np.transpose(translation)\n",
    "    return tm\n",
    "\n",
    "def transform_points(points, transf_matrix):\n",
    "    \"\"\"\n",
    "    Transform (3,N) or (4,N) points using transformation matrix.\n",
    "    \"\"\"\n",
    "    if points.shape[0] not in [3,4]:\n",
    "        raise Exception(\"Points input should be (3,N) or (4,N) shape, received {}\".format(points.shape))\n",
    "    return transf_matrix.dot(np.vstack((points[:3, :], np.ones(points.shape[1]))))[:3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def car_to_voxel_coords(points, shape, voxel_size, z_offset=0):\n",
    "    if len(shape) != 3:\n",
    "        raise Exception(\"Voxel volume shape should be 3 dimensions (x,y,z)\")\n",
    "        \n",
    "    if len(points.shape) != 2 or points.shape[0] not in [3, 4]:\n",
    "        raise Exception(\"Input points should be (3,N) or (4,N) in shape, found {}\".format(points.shape))\n",
    "\n",
    "    tm = create_transformation_matrix_to_voxel_space(shape, voxel_size, (0, 0, z_offset))\n",
    "    p = transform_points(points, tm)\n",
    "    return p\n",
    "\n",
    "def create_voxel_pointcloud(points, shape, voxel_size=(0.5,0.5,1), z_offset=0):\n",
    "\n",
    "    points_voxel_coords = car_to_voxel_coords(points.copy(), shape, voxel_size, z_offset)\n",
    "    points_voxel_coords = points_voxel_coords[:3].transpose(1,0)\n",
    "    points_voxel_coords = np.int0(points_voxel_coords)\n",
    "    \n",
    "    bev = np.zeros(shape, dtype=np.float32)\n",
    "    bev_shape = np.array(shape)\n",
    "\n",
    "    within_bounds = (np.all(points_voxel_coords >= 0, axis=1) * np.all(points_voxel_coords < bev_shape, axis=1))\n",
    "    \n",
    "    points_voxel_coords = points_voxel_coords[within_bounds]\n",
    "    coord, count = np.unique(points_voxel_coords, axis=0, return_counts=True)\n",
    "        \n",
    "    # Note X and Y are flipped:\n",
    "    bev[coord[:,1], coord[:,0], coord[:,2]] = count\n",
    "    \n",
    "    return bev\n",
    "\n",
    "def normalize_voxel_intensities(bev, max_intensity=16):\n",
    "    return (bev/max_intensity).clip(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bev_shape = (336, 336, 3)\n",
    "target_im = np.zeros(bev_shape, dtype=np.uint8)\n",
    "\n",
    "def move_boxes_to_car_space(boxes, ego_pose):\n",
    "    \"\"\"\n",
    "    Move boxes from world space to car space.\n",
    "    Note: mutates input boxes.\n",
    "    \"\"\"\n",
    "    translation = -np.array(ego_pose['translation'])\n",
    "    rotation = Quaternion(ego_pose['rotation']).inverse\n",
    "    \n",
    "    for box in boxes:\n",
    "        # Bring box to car space\n",
    "        box.translate(translation)\n",
    "        box.rotate(rotation)\n",
    "        \n",
    "def scale_boxes(boxes, factor):\n",
    "    \"\"\"\n",
    "    Note: mutates input boxes\n",
    "    \"\"\"\n",
    "    for box in boxes:\n",
    "        box.wlh = box.wlh * factor\n",
    "\n",
    "def draw_boxes(im, voxel_size, boxes, classes, z_offset=0.0):\n",
    "    for box in boxes:\n",
    "        # We only care about the bottom corners\n",
    "        corners = box.bottom_corners()\n",
    "        corners_voxel = car_to_voxel_coords(corners, im.shape, voxel_size, z_offset).transpose(1,0)\n",
    "        corners_voxel = corners_voxel[:,:2] # Drop z coord\n",
    "\n",
    "        class_color = classes.index(box.name) + 1\n",
    "        \n",
    "        if class_color == 0:\n",
    "            raise Exception(\"Unknown class: {}\".format(box.name))\n",
    "\n",
    "        cv2.drawContours(im, np.int0([corners_voxel]), 0, (class_color, class_color, class_color), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some hyperparameters we'll need to define for the system\n",
    "voxel_size = (0.4, 0.4, 1.5)\n",
    "z_offset = -2.0\n",
    "bev_shape = (336, 336, 3)\n",
    "\n",
    "# We scale down each box so they are more separated when projected into our coarse voxel space.\n",
    "box_scale = 0.8\n",
    "\n",
    "NUM_WORKERS = os.cpu_count() * 3\n",
    "\n",
    "# \"bev\" stands for birds eye view\n",
    "# test_data_folder = os.path.join(ARTIFACTS_FOLDER, \"bev_test_data\")\n",
    "test_data_folder = './artifacts/bev_test_data'\n",
    "os.makedirs(test_data_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_testing_data_for_scene(sample_token, output_folder=test_data_folder,\n",
    "                                   bev_shape=bev_shape, voxel_size=voxel_size, z_offset=z_offset,\n",
    "                                   box_scale=box_scale):\n",
    "    \"\"\"\n",
    "    Given a sample token (in a scene), output rasterized input volumes in birds-eye-view perspective.\n",
    "\n",
    "    \"\"\"  \n",
    "    sample = level5data.get(\"sample\", sample_token)\n",
    "    \n",
    "\n",
    "    sample_lidar_token = sample[\"data\"][\"LIDAR_TOP\"]\n",
    "    lidar_data = level5data.get(\"sample_data\", sample_lidar_token)\n",
    "    lidar_filepath = level5data.get_sample_data_path(sample_lidar_token)\n",
    "    \n",
    "    \n",
    "\n",
    "    ego_pose = level5data.get(\"ego_pose\", lidar_data[\"ego_pose_token\"])\n",
    "    calibrated_sensor = level5data.get(\"calibrated_sensor\", lidar_data[\"calibrated_sensor_token\"])\n",
    "    \n",
    "\n",
    "\n",
    "    global_from_car = transform_matrix(ego_pose['translation'],\n",
    "                                       Quaternion(ego_pose['rotation']), inverse=False)\n",
    "    \n",
    "\n",
    "    car_from_sensor = transform_matrix(calibrated_sensor['translation'], Quaternion(calibrated_sensor['rotation']),\n",
    "                                        inverse=False)\n",
    "    \n",
    "    \n",
    "    lidar_pointcloud = LidarPointCloud.from_file(lidar_filepath)\n",
    "    \n",
    "    lidar_pointcloud.transform(car_from_sensor)\n",
    "\n",
    "    bev = create_voxel_pointcloud(lidar_pointcloud.points, bev_shape, voxel_size=voxel_size, z_offset=z_offset)\n",
    "    bev = normalize_voxel_intensities(bev)\n",
    "\n",
    "    bev_im = np.round(bev*255).astype(np.uint8)\n",
    "\n",
    "    cv2.imwrite(os.path.join(output_folder, \"{}_input.png\".format(sample_token)), bev_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7630fed4234c22a804ad1ae6702e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27468), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for token in tqdm_notebook(sample_sub.loc[:,'Id'].values):\n",
    "    prepare_testing_data_for_scene(token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
